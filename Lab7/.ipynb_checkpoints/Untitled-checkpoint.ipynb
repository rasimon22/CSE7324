{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive 0.16140710382513662\n",
      "neutral 0.21168032786885246\n",
      "negative 0.6269125683060109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/lib/python3.7/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#prepare data and class variables\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import one_hot, text_to_word_sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "%matplotlib inline\n",
    "\n",
    "tweets_df = pd.read_csv('../tweets.csv')\n",
    "\n",
    "print('positive', len(tweets_df[tweets_df['airline_sentiment'] == 'positive'])/len(tweets_df))\n",
    "print('neutral', len(tweets_df[tweets_df['airline_sentiment'] == 'neutral'])/len(tweets_df))\n",
    "print('negative', len(tweets_df[tweets_df['airline_sentiment'] == 'negative'])/len(tweets_df))\n",
    "\n",
    "tweets_df[tweets_df['airline_sentiment'] == 'positive']['airline_sentiment'] = 0\n",
    "tweets_df[tweets_df['airline_sentiment'] == 'neutral']['airline_sentiment'] = 1\n",
    "tweets_df[tweets_df['airline_sentiment'] == 'positive']['airline_sentiment'] = 2\n",
    "\n",
    "X = tweets_df['text'].tolist()\n",
    "y = tweets_df['airline_sentiment'].tolist()\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_Y = encoder.transform(y)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "y = dummy_y\n",
    "\n",
    "vocab_size = 0\n",
    "for entry in X:\n",
    "    words = set(text_to_word_sequence(entry))\n",
    "    vocab_size += len(words)\n",
    "    \n",
    "X = [one_hot(entry, round(vocab_size*1.3)) for entry in X]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, dummy_y, random_state=42)\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=500)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=500)\n",
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding_17 (Embedding)     (None, 500, 50)           12416200  \n",
      "_________________________________________________________________\n",
      "simple_rnn_16 (SimpleRNN)    (None, 25)                1900      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 3)                 78        \n",
      "=================================================================\n",
      "Total params: 12,418,178\n",
      "Trainable params: 12,418,178\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#modeling\n",
    "\n",
    "from keras.models import Sequential, Input, Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN, LSTM, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "EMBED_SIZE = 50\n",
    "rnns = []\n",
    "input_holder = Input(shape=(X_train.shape[1], ))\n",
    "input_embed = Embedding(vocab_size,\n",
    "                       50,\n",
    "                       input_length=500)(input_holder)\n",
    "x = SimpleRNN(25, dropout=0.2, recurrent_dropout=0.2)(input_embed)\n",
    "x = Dense(3, activation='sigmoid')(x)\n",
    "rnn=Model(inputs=input_holder,outputs=x)\n",
    "rnn.compile(loss='binary_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "rnn.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding_18 (Embedding)     (None, 500, 50)           12416200  \n",
      "_________________________________________________________________\n",
      "simple_rnn_17 (SimpleRNN)    (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 12,421,403\n",
      "Trainable params: 12,421,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding_18 (Embedding)     (None, 500, 50)           12416200  \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 12,436,553\n",
      "Trainable params: 12,436,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding_18 (Embedding)     (None, 500, 50)           12416200  \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (None, 50)                15150     \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 12,431,503\n",
      "Trainable params: 12,431,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_holder = Input(shape=(X_train.shape[1], ))\n",
    "input_embed = Embedding(vocab_size,\n",
    "                       50,\n",
    "                       input_length=500)(input_holder)\n",
    "for func in [SimpleRNN, LSTM, GRU]:\n",
    "    \n",
    "    x = func(50, dropout=0.2, recurrent_dropout=0.2)(input_embed)\n",
    "    x = Dense(3, activation='sigmoid')(x)\n",
    "    rnn=Model(inputs=input_holder, outputs=x)\n",
    "    rnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(rnn.summary())\n",
    "    rnns.append(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= simple ========\n",
      "Train on 10980 samples, validate on 3660 samples\n",
      "Epoch 1/20\n",
      "10980/10980 [==============================] - 26s 2ms/step - loss: 0.8098 - acc: 0.6619 - val_loss: 0.7888 - val_acc: 0.6708\n",
      "Epoch 2/20\n",
      "10980/10980 [==============================] - 26s 2ms/step - loss: 0.7579 - acc: 0.6859 - val_loss: 0.9281 - val_acc: 0.6590\n",
      "Epoch 3/20\n",
      "10980/10980 [==============================] - 26s 2ms/step - loss: 0.7407 - acc: 0.6936 - val_loss: 0.8261 - val_acc: 0.6642\n",
      "Epoch 4/20\n",
      "10980/10980 [==============================] - 26s 2ms/step - loss: 0.7163 - acc: 0.7007 - val_loss: 0.8059 - val_acc: 0.6678\n",
      "Epoch 5/20\n",
      "10980/10980 [==============================] - 26s 2ms/step - loss: 0.6745 - acc: 0.7287 - val_loss: 0.7942 - val_acc: 0.6814\n",
      "Epoch 6/20\n",
      "10980/10980 [==============================] - 26s 2ms/step - loss: 0.6319 - acc: 0.7527 - val_loss: 0.7535 - val_acc: 0.6943\n",
      "Epoch 7/20\n",
      "10980/10980 [==============================] - 26s 2ms/step - loss: 0.5857 - acc: 0.7748 - val_loss: 0.7492 - val_acc: 0.6934\n",
      "Epoch 8/20\n",
      "10980/10980 [==============================] - 26s 2ms/step - loss: 0.5535 - acc: 0.7905 - val_loss: 0.7629 - val_acc: 0.6992\n",
      "Epoch 9/20\n",
      "10980/10980 [==============================] - 26s 2ms/step - loss: 0.5221 - acc: 0.8018 - val_loss: 0.7561 - val_acc: 0.6978\n",
      "Epoch 10/20\n",
      " 7552/10980 [===================>..........] - ETA: 7s - loss: 0.4897 - acc: 0.8175"
     ]
    }
   ],
   "source": [
    "for rnn, name in zip(rnns,['simple','lstm','gru']):\n",
    "    print('=======',name,'========')\n",
    "    rnn.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
